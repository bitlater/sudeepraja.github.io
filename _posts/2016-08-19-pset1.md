---
layout: post
title: Problem Set 1 â€“ CS224d course
published: true
---
This is my first post. Since July I have been studing online course [CS224d](http://cs224d.stanford.edu) from Stanford University about [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) in [NLP](https://en.wikipedia.org/wiki/Natural_language_processing). So this post includes implementations of some basic functions which are used in problem set 1.

## Softmax implementation


```python
import numpy as np
import random
```


```python
def softmax(x):
    x = np.array(x)
    #print x.ndim
    if x.ndim == 1:
        m = np.max(x)
        x = np.exp(x-m)/np.sum(np.exp(x-m))
    else:
        m = np.max(x, axis=1)
        m = m.reshape(m.shape[0],1)
        a = np.exp(x-m)
        b = np.sum(np.exp(x-m), axis=1)
        b = b.reshape(b.shape[0],1)
        x = a/b
    return x
```


```python
N = 2
d = 2
x = np.random.randn(N,d)
x = np.array([[1,2], [3,4]])
x = np.array([1,2])
print x.shape
#print x
softmax(x)
```

    (2,)





    array([ 0.26894142,  0.73105858])



## Sigmoid implementation


```python
def sigmoid(x):
    return 1.0/(1.0+np.exp(-x))
```


```python
sigmoid(np.array([[1, 2], [-1, -2]]))
```




    array([[ 0.73105858,  0.88079708],
           [ 0.26894142,  0.11920292]])



## Sigmoid gradient implementation


```python
def sigmoid_grad(x):
    return sigmoid(x)*(1.0-sigmoid(x))
```


```python
sigmoid_grad(np.array([[1, 2], [-1, -2]]))
```




    array([[ 0.19661193,  0.10499359],
           [ 0.19661193,  0.10499359]])



## Gradient checker implementation


```python
# First implement a gradient checker by filling in the following functions
def gradcheck_naive(f, x):
    """ 
    Gradient check for a function f 
    - f should be a function that takes a single argument and outputs the cost and its gradients
    - x is the point (numpy array) to check the gradient at
    """ 

    rndstate = random.getstate()
    random.setstate(rndstate)  
    fx, grad = f(x) # Evaluate function value at original point
    h = 1e-4

    # Iterate over all indexes in x
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        ix = it.multi_index
        
        ### try modifying x[ix] with h defined above to compute numerical gradients
        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it 
        ### possible to test cost functions with built in randomness later
        ### YOUR CODE HERE:
        old_value = x[ix]
        x[ix] += h
        random.setstate(rndstate)
        fx_high, _ = f(x)

        x[ix] = old_value - h
        random.setstate(rndstate)
        fx_low, _ = f(x)
        numgrad = (fx_high - fx_low)/(2*h)

        x[ix] = old_value
        ### END YOUR CODE

        # Compare gradients
        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))
        if reldiff > 1e-5:
            print "Gradient check failed."
            print "First gradient error found at index %s" % str(ix)
            print "Your gradient: %f \t Numerical gradient: %f" % (grad[ix], numgrad)
            return
    
        it.iternext() # Step to next dimension

    print "Gradient check passed!"
```


```python
g = lambda x: (np.sum(x ** 2), 2*x)
gradcheck_naive(g, np.array(123.456))
gradcheck_naive(g, np.random.randn(3,))
gradcheck_naive(g, np.random.randn(4,5))
```

    Gradient check passed!
    Gradient check passed!
    Gradient check passed!



```python
sigm = lambda x: (np.sum(sigmoid(x)), sigmoid_grad(x))
gradcheck_naive(sigm, np.array(234.567))
gradcheck_naive(sigm, np.random.randn(4,))
gradcheck_naive(sigm, np.random.randn(2,5))
```

    Gradient check passed!
    Gradient check passed!
    Gradient check passed!


## Neural network implementation


```python
def forward_backward_prop(data, labels, params, dimensions, debug=True):
    """ 
    Forward and backward propagation for a two-layer sigmoidal network 
    
    Compute the forward propagation and for the cross entropy cost,
    and backward propagation for the gradients for all parameters.
    """

    ### Unpack network parameters (do not modify)
    ofs = 0
    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])

    W1 = np.reshape(params[ofs:ofs + Dx * H], (Dx, H))
    ofs += Dx * H
    b1 = np.reshape(params[ofs:ofs + H], (1, H))
    ofs += H
    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))
    ofs += H * Dy
    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))

    ### YOUR CODE HERE: forward propagation
    z2 = np.dot(data,W1)+b1
    a2 = sigmoid(z2)
    z3 = np.dot(a2,W2)+b2
    #a3 = sigmoid(z3)
    a3 = softmax(z3)
    
    if debug==True:
        print 'Shape of W1 is',W1.shape
        print 'Shape of b1 is',b1.shape
        print 'Shape of W2 is',W2.shape
        print 'Shape of b2 is',b2.shape
        print 'Shape of X is',data.shape
        print 'Shape of y is',labels.shape
        print 'Shape of a2 is',a2.shape
        print 'Shape of a3 is',a3.shape
    
    def cross_entropy(x,y,n):
        return -(np.sum(y*np.log(x)))/n
    
    cost = cross_entropy(a3,labels,data.shape[0])
    ### END YOUR CODE
    
    ### YOUR CODE HERE: backward propagation
    d3 = (a3 - labels)/data.shape[0]

    if debug==True:
        print 'Shape of d3 is',d3.shape
    
    d2 = np.multiply(sigmoid_grad(z2), np.dot(d3,W2.T))

    gradW2 = np.dot(a2.T,d3)
    gradb2 = sum(d3)
    gradW1 = np.dot(data.T,d2)
    gradb1 = sum(d2)
    
    if debug==True:
        print 'Shape of gradW2 is',gradW2.shape
        print 'Shape of gradb2 is',gradb2.shape
        print 'Shape of gradW1 is',gradW1.shape
        print 'Shape of gradb1 is',gradb1.shape

    ### END YOUR CODE
    
    ### Stack gradients (do not modify)
    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), 
        gradW2.flatten(), gradb2.flatten()))
    
    return cost, grad

def sanity_check():
    """
    Set up fake data and parameters for the neural network, and test using 
    gradcheck.
    """
    print "Running sanity check..."

    N = 30
    dimensions = [15, 5, 10]
    data = np.random.randn(N, dimensions[0])   # each row will be a datum
    labels = np.zeros((N, dimensions[2]))
    for i in xrange(N):
        labels[i,random.randint(0,dimensions[2]-1)] = 1
    
    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (
        dimensions[1] + 1) * dimensions[2], )

    gradcheck_naive(lambda params: forward_backward_prop(data, labels, params,
        dimensions, debug=False), params)
    
sanity_check()    
```

    Running sanity check...
    Gradient check passed!

### Share this post 

{% include share_ya.html %}

